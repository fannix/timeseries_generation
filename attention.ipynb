{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPLs0xbwZIetgs5Xhb/rqtY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fannix/timeseries_generation/blob/master/attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtSH_ivjqXHn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "3a7e325d-e3d0-46d0-98c5-913ad7d39da4"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "\n",
        "class PeriodicSeriesDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, sequence):\n",
        "    \n",
        "    self.sequence = sequence\n",
        "    self.start_symbol = \"SOS\"\n",
        "    self.end_symbol = \"EOS\"\n",
        "    self.x = []\n",
        "    self.y = []\n",
        "    self.id2word = {i+2: w for (i, w) in enumerate(sequence)}\n",
        "    self.id2word[0] = self.start_symbol\n",
        "    self.id2word[1] = self.end_symbol\n",
        "    self.word2id = {w: i for (i, w) in self.id2word.items()}\n",
        "    seq2id = np.array([self.word2id[w] for w in sequence])\n",
        "    print(sequence)\n",
        "    place_holder = np.zeros(len(sequence) + 2, dtype=np.int)\n",
        "    place_holder[-1] = 1\n",
        "    for i in range(len(sequence)):\n",
        "      place_holder[1:-1] = np.roll(seq2id, i)\n",
        "      self.x.append(place_holder.copy())\n",
        "      place_holder[1:-1] = np.roll(seq2id, i+1)\n",
        "      self.y.append(place_holder.copy())\n",
        "      \n",
        "  def __len__(self):\n",
        "    return len(self.id2word) - 2\n",
        "\n",
        "  def onehot_seq(self, word_seq):\n",
        "    num_seq = [self.word2id[w] for w in word_seq]\n",
        "    return self.onehot_num(num_seq)\n",
        "  \n",
        "  def onehot_num(self, num_seq):\n",
        "    y = torch.LongTensor(num_seq).view(-1, 1)\n",
        "    onehot = torch.FloatTensor(len(num_seq), len(self.word2id))\n",
        "    onehot.zero_()\n",
        "    onehot.scatter_(1, y, 1)\n",
        "    return onehot\n",
        "  \n",
        "  def onecold_num(self, tensor):\n",
        "    dim_n = tensor.shape[0]\n",
        "    dim_c = tensor.shape[1]\n",
        "    onecold = tensor.argmax(dim=1)\n",
        "    return onecold\n",
        "\n",
        "  def onecold_seq(self, tensor):\n",
        "    onecold = self.onecold_num(tensor)\n",
        "    print(onecold)\n",
        "    return [self.id2word[i.item()] for i in onecold]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return  self.x[index], self.y[index]\n",
        "\n",
        "import string\n",
        "sequence = list(string.ascii_letters[:6])\n",
        "pseries = PeriodicSeriesDataset(sequence)\n",
        "for i in range(len(pseries)):\n",
        "  print(pseries[i])\n",
        "\n",
        "onehot = pseries.onehot_seq(['a', 'b', 'c'])\n",
        "print(onehot)\n",
        "onecold = pseries.onecold_seq(onehot)\n",
        "print(onecold)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['a', 'b', 'c', 'd', 'e', 'f']\n",
            "(array([0, 2, 3, 4, 5, 6, 7, 1]), array([0, 7, 2, 3, 4, 5, 6, 1]))\n",
            "(array([0, 7, 2, 3, 4, 5, 6, 1]), array([0, 6, 7, 2, 3, 4, 5, 1]))\n",
            "(array([0, 6, 7, 2, 3, 4, 5, 1]), array([0, 5, 6, 7, 2, 3, 4, 1]))\n",
            "(array([0, 5, 6, 7, 2, 3, 4, 1]), array([0, 4, 5, 6, 7, 2, 3, 1]))\n",
            "(array([0, 4, 5, 6, 7, 2, 3, 1]), array([0, 3, 4, 5, 6, 7, 2, 1]))\n",
            "(array([0, 3, 4, 5, 6, 7, 2, 1]), array([0, 2, 3, 4, 5, 6, 7, 1]))\n",
            "tensor([[0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0.]])\n",
            "tensor([2, 3, 4])\n",
            "['a', 'b', 'c']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWclGfe-E4qQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "f14c9f59-59ed-45fc-b62e-3c397ef09233"
      },
      "source": [
        "batch_size = 5\n",
        "nb_digits = 10\n",
        "y = torch.LongTensor(batch_size,1).random_() % nb_digits\n",
        "y\n",
        "y_onehot = torch.FloatTensor(batch_size, nb_digits)\n",
        "y_onehot.zero_()\n",
        "y_onehot.scatter_(1, y, 1)"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8XAH9fUGPQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "?y_onehot.scatter_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq-knOM4rWPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU7xzLWwqq9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "  def __init__(self, hidden_size, output_size):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "    self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "    self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, input, hc):\n",
        "    output = self.embedding(input)\n",
        "    output = torch.relu(output)\n",
        "    output, (hidden, cell) = self.lstm(output, hc)\n",
        "    #print(output.shape)\n",
        "    output = self.out(output)\n",
        "    return output, (hidden, cell)\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    return (torch.zeros(1, batch_size, self.hidden_size, device=device), \n",
        "            torch.zeros(1, batch_size, self.hidden_size, device=device))\n",
        "\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "    self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "\n",
        "  def forward(self, input, hc):\n",
        "    embedded = self.embedding(input)\n",
        "    output = embedded\n",
        "    output, (hidden, cell) = self.lstm(output, hc)\n",
        "    return output, hidden\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    return (torch.zeros(1, batch_size, self.hidden_size, device=device),\n",
        "            torch.zeros(1, batch_size, self.hidden_size, device=device))\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super().__init__()\n",
        "    self.encoder = EncoderRNN(input_size, hidden_size)\n",
        "    self.decoder = DecoderRNN(hidden_size, input_size)\n",
        "  \n",
        "  def forward(self, input, expected = None):\n",
        "    batch_size = input.shape[0]\n",
        "    h0, c0 = self.encoder.init_hidden(batch_size)\n",
        "    encode_output, encode_hidden = self.encoder(input, (h0, c0))\n",
        "\n",
        "    inp = torch.zeros(batch_size, dtype=torch.long)\n",
        "    inp = inp.view(batch_size, 1)\n",
        "    h = encode_hidden\n",
        "    _, c = self.decoder.init_hidden(batch_size)\n",
        "\n",
        "    #print(inp.shape)\n",
        "    output_list = []\n",
        "    while True:\n",
        "      if expected == None:\n",
        "        out, (h, c) = self.decoder(inp, (h, c))\n",
        "      # teacher forcing\n",
        "      else:\n",
        "        pass\n",
        "      output_list.append(out.squeeze(1))\n",
        "      if len(output_list) == input.shape[1] - 1:\n",
        "        break\n",
        "    # print(output_list[0].shape)\n",
        "    return torch.stack(output_list, 2)\n",
        "\n",
        "loader = DataLoader(pseries, 4)\n",
        "\n",
        "model = Seq2Seq(len(pseries.word2id), 20)\n",
        "\n",
        "criterion = torch.nn.functional.cross_entropy\n",
        "optimizer = torch.optim.RMSprop(model.parameters())\n",
        "for epoch in range(100):\n",
        "  sum_criterion = 0\n",
        "  n_instance = 0\n",
        "  for x, y in loader:\n",
        "    n_instance += x.shape[0]\n",
        "    optimizer.zero_grad()\n",
        "    res = model(x)\n",
        "    loss = criterion(res, y[:, 1:])\n",
        "    loss.backward()\n",
        "    sum_criterion += loss.item()\n",
        "    \n",
        "    optimizer.step()\n",
        "\n",
        "  print(f'{epoch}: {sum_criterion/n_instance}')\n",
        "  #print(res.shape)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9BjQYY2dOxq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d4eb5389-5601-4f40-ce44-4d68c40b1fd1"
      },
      "source": [
        "res.argmax(dim=1)"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3, 4, 5, 6, 7, 2, 1],\n",
              "        [2, 3, 4, 5, 6, 7, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhxDoDu1dbGf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "21dd8917-90f5-48da-9856-81f04e78731f"
      },
      "source": [
        "y"
      ],
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 3, 4, 5, 6, 7, 2, 1],\n",
              "        [0, 2, 3, 4, 5, 6, 7, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUqDzcBJMkfZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "86bf7601-bdda-4b7f-858f-99e30b8a10fe"
      },
      "source": [
        "model(\n",
        "    torch.LongTensor(\n",
        "    [[0, 2, 3, 4, 5, 6, 7, 1],\n",
        "     [0, 4, 5, 6, 7, 2, 3, 1]\n",
        "     ])\n",
        ").argmax(dim=1)"
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[7, 2, 3, 4, 5, 6, 1],\n",
              "        [3, 4, 5, 6, 7, 2, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 237
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhQsevTDMy8C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9c2320f3-e669-4ee5-aad7-5cece40a9e97"
      },
      "source": [
        "model(\n",
        "    torch.LongTensor(\n",
        "    [[0, 2, 3, 4, 5, 6, 1],\n",
        "     [0, 4, 5, 6, 7, 2, 1]\n",
        "     ])\n",
        ").argmax(dim=1)"
      ],
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[6, 7, 2, 3, 4, 5],\n",
              "        [2, 3, 4, 5, 6, 7]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 238
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wcs7mqlYliy3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b2efce0-c8d3-4399-9ae2-eb4043225820"
      },
      "source": [
        "criterion(res, y[:, 1:])"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0118, grad_fn=<NllLoss2DBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44c0tMmZ_bf3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c4c4e16-d1ae-42d6-d3f5-6daf26225f61"
      },
      "source": [
        "y[:, 1:].shape"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 7])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uFMpTxe_46M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "241b2879-c145-43ff-e9a5-9d5e52f62233"
      },
      "source": [
        "res.shape"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 8, 7])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpobzVRI_NZJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef4c5b02-6dc3-4f68-f717-db7039d088c5"
      },
      "source": [
        "criterion(torch.rand((1, 8, 7)), y[1:, 1:])"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.1528)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb-jhDJ-0Cpy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8bd43a00-68b5-4b52-d8ba-45f39c0bd36e"
      },
      "source": [
        "len(pseries)"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxBs9aSd83Dl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1796e91a-b2a1-41d1-fef7-ff59ef8e2e84"
      },
      "source": [
        "input = torch.randn(4, 8, requires_grad=True)\n",
        "target = torch.randint(8, (4,), dtype=torch.int64)\n",
        "\n",
        "torch.nn.functional.cross_entropy(input, target)"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.0650, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEgPPwr0f5fa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk0GKtWAf9IM",
        "colab_type": "text"
      },
      "source": [
        "Attention LSTM Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5-o01AZGf6kv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2acc956d-52b5-4d47-decc-31ff503bcc9e"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class AttenEncoderRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "    self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "\n",
        "  def forward(self, input, hc):\n",
        "    embedded = self.embedding(input)\n",
        "    output = embedded\n",
        "    output, (hidden, cell) = self.lstm(output, hc)\n",
        "    return output, hidden\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    return (torch.zeros(1, batch_size, self.hidden_size, device=device),\n",
        "            torch.zeros(1, batch_size, self.hidden_size, device=device))\n",
        "\n",
        "class AttenDecoderRNN(nn.Module):\n",
        "  def __init__(self, hidden_size, output_size):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "    self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "    self.out = nn.Linear(hidden_size, output_size)\n",
        "    self.combine = nn.Linear(2 * hidden_size, hidden_size)\n",
        "\n",
        "  def forward(self, input, hc, encode_out):\n",
        "    embed = self.embedding(input)\n",
        "    attn = attention(embed, encode_out, encode_out)\n",
        "    comb = self.combine(torch.cat([embed, attn[0]], -1))\n",
        "    output = torch.relu(comb)\n",
        "    output, (hidden, cell) = self.lstm(output, hc)\n",
        "    #print(output.shape)\n",
        "    output = self.out(output)\n",
        "    return output, (hidden, cell)\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    return (torch.zeros(1, batch_size, self.hidden_size, device=device), \n",
        "            torch.zeros(1, batch_size, self.hidden_size, device=device))\n",
        "\n",
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "             / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = torch.nn.functional.softmax(scores, dim = -1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn\n",
        "\n",
        "class AttenSeq2Seq(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super().__init__()\n",
        "    self.encoder = AttenEncoderRNN(input_size, hidden_size)\n",
        "    self.decoder = AttenDecoderRNN(hidden_size, input_size)\n",
        "  \n",
        "  def forward(self, input, expected = None):\n",
        "    batch_size = input.shape[0]\n",
        "    h0, c0 = self.encoder.init_hidden(batch_size)\n",
        "    encode_output, encode_hidden = self.encoder(input, (h0, c0))\n",
        "\n",
        "    inp = torch.zeros(batch_size, dtype=torch.long)\n",
        "    inp = inp.view(batch_size, 1)\n",
        "    h = encode_hidden\n",
        "    _, c = self.decoder.init_hidden(batch_size)\n",
        "\n",
        "    #print(inp.shape)\n",
        "    output_list = []\n",
        "    while True:\n",
        "      if expected == None:\n",
        "        out, (h, c) = self.decoder(inp, (h, c), encode_output)\n",
        "      # teacher forcing\n",
        "      else:\n",
        "        pass\n",
        "      output_list.append(out.squeeze(1))\n",
        "      if len(output_list) == input.shape[1] - 1:\n",
        "        break\n",
        "    # print(output_list[0].shape)\n",
        "    return torch.stack(output_list, 2)\n",
        "\n",
        "loader = DataLoader(pseries, 4)\n",
        "\n",
        "model = AttenSeq2Seq(len(pseries.word2id), 20)\n",
        "\n",
        "criterion = torch.nn.functional.cross_entropy\n",
        "optimizer = torch.optim.RMSprop(model.parameters())\n",
        "for epoch in range(100):\n",
        "  sum_criterion = 0\n",
        "  n_instance = 0\n",
        "  for x, y in loader:\n",
        "    n_instance += x.shape[0]\n",
        "    optimizer.zero_grad()\n",
        "    res = model(x)\n",
        "    loss = criterion(res, y[:, 1:])\n",
        "    loss.backward()\n",
        "    sum_criterion += loss.item()\n",
        "    \n",
        "    optimizer.step()\n",
        "\n",
        "  print(f'{epoch}: {sum_criterion/n_instance}')\n",
        "  #print(res.shape)\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: 0.689409097035726\n",
            "1: 0.7288676102956136\n",
            "2: 0.6594499150911967\n",
            "3: 0.6526718934377035\n",
            "4: 0.647529403368632\n",
            "5: 0.6480785409609476\n",
            "6: 0.640583872795105\n",
            "7: 0.6298904816309611\n",
            "8: 0.6232078870137533\n",
            "9: 0.627415398756663\n",
            "10: 0.6003888845443726\n",
            "11: 0.553717037041982\n",
            "12: 0.49800435702006024\n",
            "13: 0.5532203912734985\n",
            "14: 0.486881156762441\n",
            "15: 0.430888573328654\n",
            "16: 0.3748004535833995\n",
            "17: 0.3112785418828328\n",
            "18: 0.2699754536151886\n",
            "19: 0.236559788386027\n",
            "20: 0.2086629569530487\n",
            "21: 0.18467976649602255\n",
            "22: 0.17235846320788065\n",
            "23: 0.18158993124961853\n",
            "24: 0.15911189218362173\n",
            "25: 0.13173526028792062\n",
            "26: 0.12111049393812816\n",
            "27: 0.11030443509419759\n",
            "28: 0.09692167739073436\n",
            "29: 0.08522748202085495\n",
            "30: 0.07964568585157394\n",
            "31: 0.09170701851447423\n",
            "32: 0.15719732642173767\n",
            "33: 0.09743593633174896\n",
            "34: 0.07188417514165242\n",
            "35: 0.06989020109176636\n",
            "36: 0.07707143078247707\n",
            "37: 0.06446439524491628\n",
            "38: 0.0520340030392011\n",
            "39: 0.04762141406536102\n",
            "40: 0.04898884892463684\n",
            "41: 0.046395041048526764\n",
            "42: 0.06029867132504781\n",
            "43: 0.08718528101841609\n",
            "44: 0.099012091755867\n",
            "45: 0.0666719526052475\n",
            "46: 0.04657237480084101\n",
            "47: 0.042339026927948\n",
            "48: 0.039681228498617806\n",
            "49: 0.03692979986468951\n",
            "50: 0.03345286597808202\n",
            "51: 0.03065650537610054\n",
            "52: 0.027989808470010757\n",
            "53: 0.02582545205950737\n",
            "54: 0.02312430242697398\n",
            "55: 0.02090246044099331\n",
            "56: 0.019634888817866642\n",
            "57: 0.018070728207627933\n",
            "58: 0.017123137911160786\n",
            "59: 0.016122007742524147\n",
            "60: 0.015203927954037985\n",
            "61: 0.014483485370874405\n",
            "62: 0.013738999764124552\n",
            "63: 0.01310700923204422\n",
            "64: 0.01250458632906278\n",
            "65: 0.01193785791595777\n",
            "66: 0.011433707550168037\n",
            "67: 0.010946996820469698\n",
            "68: 0.010502521879971027\n",
            "69: 0.010083009799321493\n",
            "70: 0.009687618042031923\n",
            "71: 0.00932087500890096\n",
            "72: 0.008971905646224817\n",
            "73: 0.008645785972476006\n",
            "74: 0.008336810395121574\n",
            "75: 0.008044776506721973\n",
            "76: 0.007769295324881871\n",
            "77: 0.0075075409064690275\n",
            "78: 0.0072600990533828735\n",
            "79: 0.007024824619293213\n",
            "80: 0.006801454660793145\n",
            "81: 0.006589061891039212\n",
            "82: 0.006386797254284223\n",
            "83: 0.006194208438197772\n",
            "84: 0.006010490159193675\n",
            "85: 0.005835194451113542\n",
            "86: 0.005667749792337418\n",
            "87: 0.0055076745338737965\n",
            "88: 0.00535454119866093\n",
            "89: 0.005207912841190894\n",
            "90: 0.00506740544612209\n",
            "91: 0.0049324072897434235\n",
            "92: 0.004802857680867116\n",
            "93: 0.0046786293387413025\n",
            "94: 0.004559594516952832\n",
            "95: 0.004445108740280072\n",
            "96: 0.004335024859756231\n",
            "97: 0.0042289312308033305\n",
            "98: 0.004126830181727807\n",
            "99: 0.004028252170731624\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a4V3iNam9aG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "5a6f3175-bfb3-432a-cccb-4e0e23777647"
      },
      "source": [
        "a = torch.randn((4, 1, 8))\n",
        "b = torch.randn((4, 1, 8))\n",
        "c = torch.cat([a, b], 1)\n",
        "\n",
        "c[:, 1:, :] == b"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[True, True, True, True, True, True, True, True]],\n",
              "\n",
              "        [[True, True, True, True, True, True, True, True]],\n",
              "\n",
              "        [[True, True, True, True, True, True, True, True]],\n",
              "\n",
              "        [[True, True, True, True, True, True, True, True]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    }
  ]
}