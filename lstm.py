# -*- coding: utf-8 -*-
"""lstm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zWBxN73EvzASKjwb6nSpHGJcpVpvgXbl
"""

import matplotlib.pyplot as plt

import numpy as np

x = np.linspace(0, 7, 20)
y = np.sin(x)

plt.plot(x[:], y[:])

import torch
import numpy as np
class SeriesDataset(torch.utils.data.Dataset):

  def __init__(self, series, input_size, output_size):
    super().__init__()
    self.input_window = []
    self.output_window = []

    for e in series:
      if len(e) < input_size + output_size:
        continue
      input_begin = 0
      while True:
        input_end = input_begin + input_size
        output_begin = input_end
        output_end = output_begin + output_size
        if output_end > len(e):
          break
        else:
          self.input_window.append(e[input_begin:input_end])
          self.output_window.append(e[output_begin:output_end])
          input_begin += 1

  def __len__(self):
    return len(self.input_window)
  
  def __getitem__(self, index):
    return np.array(self.input_window[index], np.float), np.array(self.output_window[index], np.float)

series = [list(range(20)), list(range(15))]

dataset = SeriesDataset(series, 6, 4)
for i in range(len(dataset)):
  print(dataset[i])

import torch
from torch import nn
class LSTMModel(nn.Module):

  def __init__(self, hidden_size=10, output_window=3):
    super().__init__()
    feature_size = 1
    self.lstm = nn.LSTM(feature_size, hidden_size, 1, batch_first=True)
    self.linear = nn.Linear(hidden_size, output_window)
  
  def forward(self, x):
    out, (h, c) = self.lstm(x)
    # add residual
    res = self.linear(h) + x[:, -1, :]
    return res

input_size = 10
output_size = 1
x = np.linspace(0, 100, 500)
#y = np.sin(x)
y = x
sin_dataset = SeriesDataset([y], input_size, output_size)
model = LSTMModel(hidden_size = 20, output_window=output_size)

optimizer = torch.optim.Adam(model.parameters())
loss = torch.nn.functional.mse_loss

sin_loader = torch.utils.data.DataLoader(sin_dataset, 8)

for epoch in range(50):
  cum_loss = 0
  for i, (x, y) in enumerate(sin_loader):
    optimizer.zero_grad()
    x = x.unsqueeze(2).float()
    out = model(x)
    actual = out.squeeze(0)
    cost = loss(actual, y.float())
    cum_loss += cost.item()
    cost.backward()
    optimizer.step()

  print(f"{epoch}: {cum_loss}")

x[:, -2, :]

actual

plt.plot(range(output_size), y[1, :])

plt.plot(range(output_size), actual.detach().numpy()[2, :])

loss(actual, y)

len(sin_dataset)

model(torch.linspace(0, 500, 100).unsqueeze(0).unsqueeze(2))

torch.linspace(0, 10, 100).unsqueeze(0).unsqueeze(2).shape

"""# time series generation

1. https://github.com/osm3000/sequence_generation_pytorch
2. https://discuss.pytorch.org/t/lstm-time-series-prediction/4832/30
"""

import numpy as np
import matplotlib.pyplot as plt
import torch
from torch.autograd import Variable

def get_batch(X, Y, i, batch_size=32):
    min_len = min(batch_size, len(X) - 1 - i)
    data = X[i:i + min_len, :]
    target = Y[i:i + min_len, :]
    return data, target

def sine_1(X, signal_freq=60., add_noise=False, noise_range=(-0.1, 0.1)):
    if add_noise:
        clean_signal = np.sin(2 * np.pi * (X) / signal_freq)
        noisy_signal = clean_signal + np.random.uniform(noise_range[0], noise_range[1], size=clean_signal.shape)
        return noisy_signal
    else:
        return np.sin(2 * np.pi * (X) / signal_freq)


def sine_2(X, signal_freq=60., add_noise=False, noise_range=(-0.1, 0.1)):
    if add_noise:
        clean_signal = (np.sin(2 * np.pi * (X) / signal_freq) + np.sin(2 * 2 * np.pi * (X) / signal_freq)) / 2.0
        noisy_signal = clean_signal + np.random.uniform(noise_range[0], noise_range[1], size=clean_signal.shape)
        return noisy_signal
    else:
        return (np.sin(2 * np.pi * (X) / signal_freq) + np.sin(2 * 2 * np.pi * (X) / signal_freq)) / 2.0


def sine_3(X, signal_freq=60., add_noise=False, noise_range=(-0.1, 0.1)):
    if add_noise:
        clean_signal = (np.sin(2 * np.pi * (X) / signal_freq) + np.sin(2 * 2 * np.pi * (X) / signal_freq) + np.sin(
        2 * 3 * np.pi * (X) / signal_freq)) / 3.0
        noisy_signal = clean_signal + np.random.uniform(noise_range[0], noise_range[1], size=clean_signal.shape)
        return noisy_signal
    else:
        return (np.sin(2 * np.pi * (X) / signal_freq) + np.sin(2 * 2 * np.pi * (X) / signal_freq) + np.sin(
            2 * 3 * np.pi * (X) / signal_freq)) / 3.0


def generate_data(data_fn=sine_1, nb_samples=10000, seq_len=100, signal_freq=60., add_noise=False):
    x_data = np.arange(nb_samples)
    y_data = data_fn(x_data, signal_freq=signal_freq, add_noise=add_noise)
    plt.figure("Synthetic data", figsize=(15, 10))
    plt.title("Synthetic data")
    plt.plot(x_data[:400], y_data[:400])
    plt.savefig("synthetic_data.png")
    plt.close()

    X = []
    y = []
    # for i in range(0, y_data.shape[0] - 1, seq_len):
    for i in range(0, y_data.shape[0] - 1):
        if i + seq_len < y_data.shape[0]:
            X.append(y_data[i:i + seq_len])
            y.append(y_data[i + 1:i + seq_len + 1])  # next sequence (including the next point in the last)
            # y.append(y_data[i + seq_len])  # next point only
    X = np.array(X)
    y = np.array(y)

    plt.figure(figsize=(15, 10))
    n = np.squeeze(X[:, -1])
    plt.plot(n, "go")
    plt.show()

    nb_seq = X.shape[0]
    # 70% training, 10% validation, 20% testing
    train_sep = int(nb_seq * 0.7)
    val_sep = train_sep + int(nb_seq * 0.1)

    X_train = Variable(torch.from_numpy(X[:train_sep, :]).float()).cuda()
    y_train = Variable(torch.from_numpy(y[:train_sep, :]).float()).cuda()

    X_val = Variable(torch.from_numpy(X[train_sep:val_sep, :]).float()).cuda()
    y_val = Variable(torch.from_numpy(y[train_sep:val_sep, :]).float()).cuda()

    X_test = Variable(torch.from_numpy(X[val_sep:, :]).float()).cuda()
    y_test = Variable(torch.from_numpy(y[val_sep:, :]).float()).cuda()

    print ("X_train size = {}, X_val size = {}, X_test size = {}".format(X_train.size(), X_val.size(), X_test.size()))

    return X_train, X_val, X_test, y_train, y_val, y_test

generate_data()

x = np.linspace(0, 60, 500)
plt.plot(x, sine_3(x))

"""# Pytorch time series example

https://github.com/pytorch/examples/tree/master/time_sequence_prediction
"""

import numpy as np
import torch

np.random.seed(2)

T = 20
L = 1000
N = 100

x = np.empty((N, L), 'int64')
x[:] = np.array(range(L)) + np.random.randint(-4 * T, 4 * T, N).reshape(N, 1)
data = np.sin(x / 1.0 / T).astype('float64')
torch.save(data, open('traindata.pt', 'wb'))

x = np.empty((N, L), 'int64')
x[:] = np.array(range(L))
x

from __future__ import print_function
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

class Sequence(nn.Module):
    def __init__(self):
        super(Sequence, self).__init__()
        self.lstm1 = nn.LSTMCell(1, 51)
        self.lstm2 = nn.LSTMCell(51, 51)
        self.linear = nn.Linear(51, 1)

    def forward(self, input, future = 0):
        outputs = []
        h_t = torch.zeros(input.size(0), 51, dtype=torch.double)
        c_t = torch.zeros(input.size(0), 51, dtype=torch.double)
        h_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)
        c_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)

        for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):
            h_t, c_t = self.lstm1(input_t, (h_t, c_t))
            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))
            output = self.linear(h_t2)
            outputs += [output]
        for i in range(future):# if we should predict the future
            h_t, c_t = self.lstm1(output, (h_t, c_t))
            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))
            output = self.linear(h_t2)
            outputs += [output]
        outputs = torch.stack(outputs, 1).squeeze(2)
        return outputs


if __name__ == '__main__':
    # set random seed to 0
    np.random.seed(0)
    torch.manual_seed(0)
    # load data and make training set
    data = torch.load('traindata.pt')
    input = torch.from_numpy(data[3:, :-1])
    target = torch.from_numpy(data[3:, 1:])
    test_input = torch.from_numpy(data[:3, :-1])
    test_target = torch.from_numpy(data[:3, 1:])
    # build the model
    seq = Sequence()
    seq.double()
    criterion = nn.MSELoss()
    # use LBFGS as optimizer since we can load the whole data to train
    optimizer = optim.LBFGS(seq.parameters(), lr=0.8)
    #begin to train
    for i in range(15):
        print('STEP: ', i)
        def closure():
            optimizer.zero_grad()
            out = seq(input)
            loss = criterion(out, target)
            print('loss:', loss.item())
            loss.backward()
            return loss
        optimizer.step(closure)
        # begin to predict, no need to track gradient here
        with torch.no_grad():
            future = 1000
            pred = seq(test_input, future=future)
            loss = criterion(pred[:, :-future], test_target)
            print('test loss:', loss.item())
            y = pred.detach().numpy()
        # draw the result
        plt.figure(figsize=(30,10))
        plt.title('Predict future values for time sequences\n(Dashlines are predicted values)', fontsize=30)
        plt.xlabel('x', fontsize=20)
        plt.ylabel('y', fontsize=20)
        plt.xticks(fontsize=20)
        plt.yticks(fontsize=20)
        def draw(yi, color):
            plt.plot(np.arange(input.size(1)), yi[:input.size(1)], color, linewidth = 2.0)
            plt.plot(np.arange(input.size(1), input.size(1) + future), yi[input.size(1):], color + ':', linewidth = 2.0)
        draw(y[0], 'r')
        draw(y[1], 'g')
        draw(y[2], 'b')
        plt.savefig('predict%d.pdf'%i)
        plt.close()

# Commented out IPython magic to ensure Python compatibility.
# %ls

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cp *.pdf /content/drive/"My Drive"